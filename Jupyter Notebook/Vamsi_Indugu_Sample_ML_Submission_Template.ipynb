{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -    Netflix Movies and TV Shows Clustering\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Unsupervised\n",
        "##### **Contribution**    - Individual\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project embarks on an in-depth analysis of Netflix content, leveraging a comprehensive dataset titled \"NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv.\" This dataset encapsulates a rich collection of metadata pertaining to movies and TV shows available on the popular streaming platform, including details such as title, director, cast, country of production, release year, rating, duration, and most crucially, a textual description and listed genres. The primary objective of this initiative is to uncover inherent groupings and similarities within Netflix's vast content library through unsupervised machine learning techniques. By identifying natural clusters of movies and TV shows, we aim to transform raw data into actionable insights, providing a structured understanding of the content landscape.\n",
        "\n",
        "The core problem addressed by this project revolves around the sheer volume and diversity of content on Netflix. For both the platform and its users, navigating this extensive library efficiently can be a challenge. Specifically, Netflix faces hurdles in optimizing content discovery, personalizing user experiences, and strategically expanding its content portfolio. Users often struggle with content overload, making it difficult to find new shows or movies that align with their specific tastes beyond basic genre classifications. Furthermore, Netflix’s content acquisition and marketing teams require a data-driven approach to identify trends, popular themes, and content gaps to maintain a competitive edge. This project seeks to solve these challenges by segmenting content into meaningful, interpretable clusters, thus simplifying content management and enhancing user engagement.\n",
        "\n",
        "The brief solutions stemming from this clustering endeavor promise significant value to various stakeholders. For Netflix, these clusters can serve as the backbone for a more sophisticated content recommendation engine, moving beyond simple collaborative filtering to suggest titles based on nuanced thematic and descriptive similarities. This would lead to higher user satisfaction and retention. Moreover, the insights derived from cluster characteristics—such as dominant genres, recurring themes, or prominent directors within a cluster—can directly inform Netflix's content acquisition strategy, enabling them to identify and invest in content that resonates with specific, profitable audience segments or fills existing gaps in their offerings. From a marketing perspective, understanding these content groupings allows for highly personalized promotional campaigns, targeting users with content categories most relevant to their inferred preferences. For the user, the practical benefit is a vastly improved discovery experience, where they can effortlessly explore curated collections of content, leading to reduced decision fatigue and increased viewing pleasure.\n",
        "\n",
        "To achieve an optimized solution, a multi-faceted approach was meticulously followed. The initial phase involved rigorous Exploratory Data Analysis (EDA) to understand data distributions, identify anomalies, and quantify missing values across all features. This was followed by comprehensive data cleaning, particularly for textual fields like description, director, and cast. This involved standard natural language processing (NLP) steps such as lowercasing, punctuation removal, stop word elimination, and lemmatization to standardize text and enhance its informational density. Categorical features like listed_in (genres) and country were carefully processed to extract meaningful signals. The cleaned textual features were then combined and transformed into numerical representations using TF-IDF (Term Frequency-Inverse Document Frequency) vectorization. TF-IDF was chosen over simpler methods like Bag of Words because it effectively weights words based on their importance not just within a document but across the entire corpus, giving more prominence to distinctive terms that truly differentiate content. Given the high dimensionality inherent in TF-IDF outputs, dimensionality reduction techniques (e.g., PCA or SVD) were applied to compress the feature space while preserving crucial information, which significantly improves the efficiency and interpretability of subsequent clustering. For modeling, at least two distinct clustering algorithms were employed, typically K-Means due to its efficiency for large datasets and DBSCAN or Agglomerative Hierarchical Clustering for their ability to discover non-spherical clusters or reveal hierarchical relationships. The optimal number of clusters for K-Means was determined using methods like the Elbow Method and Silhouette Score. Finally, the generated clusters were meticulously analyzed and interpreted by examining the most frequent words and metadata elements within each group, allowing for the assignment of meaningful labels and providing actionable insights into Netflix's content ecosystem."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The core problem this project addresses is the challenge Netflix faces in organizing and recommending its vast and diverse content library. For both the platform and its users, efficient content discovery is hampered by sheer volume. This project aims to mitigate content overload for users and provide Netflix with a data-driven method to optimize content acquisition, personalize recommendations, and enhance overall user engagement by uncovering natural groupings within their movies and TV shows."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "# Core data manipulation and numerical operations\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Natural Language Processing (NLP)\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "# Make sure to download necessary NLTK data if you haven't already:\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('omw-1.4') # Open Multilingual Wordnet\n",
        "\n",
        "from scipy import stats #for statistical tests\n",
        "from scipy.stats import chi2_contingency # for chi-square test\n",
        "import statsmodels.api as sm # For OLS regression\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud # Install with: pip install wordcloud\n",
        "import re # For regex operations on text\n",
        "\n",
        "# Machine Learning - Feature Extraction (TF-IDF)\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram\n",
        "\n",
        "# Machine Learning - Dimensionality Reduction\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA, TruncatedSVD # PCA for dense, SVD for sparse matrices\n",
        "\n",
        "# Machine Learning - Clustering Algorithms\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
        "\n",
        "# Machine Learning - Evaluation Metrics (for clustering)\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "\n",
        "# Utility for warnings and logging\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set options for displaying dataframes (optional)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "# Define the Google Drive file ID\n",
        "google_drive_file_id = '1xJGllnE12mAggLuRo8b0oNSshUlG8GvF'\n",
        "download_url = f'https://drive.google.com/uc?export=download&id={google_drive_file_id}'\n",
        "\n",
        "# Load the dataset from the Google Drive direct download URL\n",
        "df = pd.read_csv(download_url)"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "print(df.info())"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "# Set the style for the plot (optional, but makes it look nicer)\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Create a heatmap of missing values\n",
        "plt.figure(figsize=(12, 6)) # Adjust figure size for better readability\n",
        "sns.heatmap(df.isnull(), cbar=False, cmap='viridis') # 'viridis' is a good colormap\n",
        "\n",
        "# Add a title to the plot\n",
        "plt.title('Missing Values Heatmap', fontsize=16)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n",
        "print(\"In the heatmap above, yellow lines indicate missing values for the respective columns.\")\n",
        "print(\"Each row represents a record, and each column represents a feature.\")"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset contains a comprehensive collection of information about movies and TV shows available on Netflix, intended for a clustering project. Upon initial inspection, the dataset consists of 7,787 rows and 12 columns, each providing specific details about a Netflix title.\n",
        "\n",
        "The columns present in the dataset include:\n",
        "\n",
        "* **show_id**: A unique identifier for each show.\n",
        "* **type**: Indicates whether the content is a 'Movie' or 'TV Show'.\n",
        "* **type**: Indicates whether the content is a 'Movie' or 'TV Show'.\n",
        "* **title**: The title of the content.\n",
        "* **director**: The director(s) of the content.\n",
        "* **cast**: The main actors/actresses in the content.\n",
        "* **country**: The country or countries where the content was produced.\n",
        "* **date_added**: The date the content was added to Netflix.\n",
        "* **release_year**: The original release year of the content.\n",
        "* **rating**: The maturity rating of the content (e.g., 'TV-MA', 'R').\n",
        "* **duration**: The duration of the content (e.g., '90 min' for movies, '1 Season' for TV shows).\n",
        "* **listed_in**: The categories or genres the content falls under.\n",
        "* **description**: A brief summary or synopsis of the content.\n",
        "\n",
        "\n",
        "During the initial data exploration, it was observed that there are no duplicate rows in the dataset. However, missing values are present in several key columns. Notably, the director, cast, country, date_added, and rating columns have a significant number of null entries. The description and title columns appear to be complete. Understanding these missing values is crucial for the data cleaning and preprocessing steps in the project, as they will need to be handled appropriately (e.g., imputation or removal) before applying machine learning algorithms for clustering. The type, listed_in, description, title, director, and cast fields are particularly important for text-based clustering."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset comprises 12 key variables (columns), each providing distinct information about a Netflix title. show_id serves as a unique identifier for each entry. type categorizes content as either a 'Movie' or 'TV Show'. title is the name of the content. director and cast list the individuals involved in production and acting, respectively. country indicates the nation(s) of origin. date_added records when the title became available on Netflix, while release_year specifies its original production year. rating denotes the maturity level (e.g., TV-MA, R). duration provides the length, varying for movies (minutes) and TV shows (seasons). listed_in details the genres, and description offers a brief summary of the plot or theme. These variables collectively form a rich basis for content understanding and clustering."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "# Iterate through each column in the DataFrame\n",
        "for column in df.columns:\n",
        "    unique_count = df[column].nunique()\n",
        "    print(f\"Column '{column}': {unique_count} unique values\")"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "# Handling Missing Values\n",
        "for col in ['director', 'cast', 'country']:\n",
        "    df[col].fillna(f'Unknown {col.capitalize()}', inplace=True)\n",
        "for col in ['date_added', 'rating']:\n",
        "    if df[col].isnull().any():\n",
        "        df[col].fillna(df[col].mode()[0], inplace=True)\n",
        "\n",
        "# Feature Engineering: Combining Textual Features\n",
        "df_wrangled = df.copy()\n",
        "text_cols = ['title', 'director', 'cast', 'country', 'listed_in', 'description', 'type']\n",
        "for col in text_cols:\n",
        "    df_wrangled[col] = df_wrangled[col].astype(str)\n",
        "\n",
        "df_wrangled['combined_features'] = (\n",
        "    df_wrangled['title'] + ' ' +\n",
        "    df_wrangled['director'] + ' ' +\n",
        "    df_wrangled['cast'] + ' ' +\n",
        "    df_wrangled['country'] + ' ' +\n",
        "    df_wrangled['listed_in'] + ' ' +\n",
        "    df_wrangled['description'] + ' ' +\n",
        "    df_wrangled['type']\n",
        ")"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the selected code, two primary manipulations are performed:\n",
        "\n",
        "Handling Missing Values:\n",
        "\n",
        "For director, cast, and country, missing values are filled with Unknown [ColumnName] (e.g., 'Unknown Director'). This approach maintains the rows without losing information and categorizes missing entries distinctly.\n",
        "For date_added and rating, missing values are imputed with the mode (most frequent value). This is suitable for categorical data and helps preserve the statistical distribution.\n",
        "Feature Engineering (combined_features):\n",
        "\n",
        "A new column, combined_features, is created by concatenating the textual content of title, director, cast, country, listed_in, description, and type. All these columns are first explicitly converted to string type to ensure smooth concatenation.\n",
        "The insight from these manipulations is that the dataset is now transformed into a state ready for advanced text processing and machine learning, particularly for clustering. By handling missing data and creating a consolidated textual feature, we provide a rich, unified input for algorithms like TF-IDF, enabling them to discover latent relationships within the Netflix content based on its descriptive attributes."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set plotting style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 7) # Default figure size\n",
        "\n",
        "# Apply missing value handling directly to df for visualization purposes\n",
        "for col in ['director', 'cast', 'country']:\n",
        "    df[col].fillna(f'Unknown {col.capitalize()}', inplace=True)\n",
        "for col in ['date_added', 'rating']:\n",
        "    if df[col].isnull().any():\n",
        "        df[col].fillna(df[col].mode()[0], inplace=True)"
      ],
      "metadata": {
        "id": "CzLvxC15_vE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "\n",
        "# --- Chart 1: Distribution of Content Types (Movies vs. TV Shows) ---\n",
        "plt.figure(figsize=(7, 5))\n",
        "sns.countplot(x='type', data=df, palette='viridis', hue='type', legend=False)\n",
        "plt.title('1. Distribution of Content Types (Movies vs. TV Shows)', fontsize=16)\n",
        "plt.xlabel('Content Type', fontsize=12)\n",
        "plt.ylabel('Count', fontsize=12)\n",
        "plt.xticks(fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "for container in plt.gca().containers:\n",
        "    plt.gca().bar_label(container, fmt='%d', fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar chart is ideal for comparing the counts of discrete categories. It provides a clear, immediate visual comparison of the volume of Movies versus TV Shows."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart reveals the proportion of movies to TV shows on Netflix. For instance, if movies significantly outnumber TV shows, it highlights Netflix's primary content focus."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Business Impact:\n",
        "\n",
        "Positive: If Netflix aims for a movie-centric library, a higher movie count confirms strategy. It helps content acquisition teams allocate resources effectively.\n",
        "\n",
        "Negative: If the balance is heavily skewed and audience demand suggests diversification, this imbalance might indicate a missed opportunity or declining subscriber engagement for the less represented type, potentially leading to churn if viewers prefer the under-resourced content type."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "\n",
        "# --- Chart 2: Proportion of Content Types (Movies vs. TV Shows) ---\n",
        "plt.figure(figsize=(8, 8))\n",
        "type_counts = df['type'].value_counts()\n",
        "plt.pie(type_counts, labels=type_counts.index, autopct='%1.1f%%', startangle=90, colors=sns.color_palette('pastel'))\n",
        "plt.title('2. Proportion of Content Types (Movies vs. TV Shows)', fontsize=16)\n",
        "plt.axis('equal') # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pie chart is effective for showing parts of a whole, providing a quick visual of the percentage split between movies and TV shows."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Directly quantifies the percentage share of each content type in Netflix's catalog, reinforcing the numerical distribution from the bar chart."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: Confirms content strategy alignment with overall platform vision (e.g., if Netflix positions itself as a movie hub). Helps marketing tailor promotional messages.\n",
        "\n",
        "Negative: If market trends indicate growing preference for TV series, and Netflix's library disproportionately favors movies, it could signify a strategic mismatch leading to subscriber dissatisfaction and potential growth stagnation in the long run."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "\n",
        "# --- Chart 3: Top 15 Countries by Content Count ---\n",
        "# Handle multiple countries in 'country' column\n",
        "df_countries = df['country'].str.split(', ').explode()\n",
        "top_countries = df_countries.value_counts().head(15)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(x=top_countries.values, y=top_countries.index, palette='crest')\n",
        "plt.title('3. Top 15 Countries by Content Count on Netflix', fontsize=16)\n",
        "plt.xlabel('Number of Titles', fontsize=12)\n",
        "plt.ylabel('Country', fontsize=12)\n",
        "plt.xticks(fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "for index, value in enumerate(top_countries.values):\n",
        "    plt.text(value, index, str(value), ha='left', va='center', fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Horizontal bar charts are excellent for displaying rankings, especially when category labels are long, as they are with country names."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identifies the leading countries contributing content to Netflix. The US typically dominates, but this shows the presence of other significant global content hubs."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: Highlights key production partners and regions. Informs content acquisition about successful markets for content sourcing or expansion.\n",
        "\n",
        "Negative: Over-reliance on one or a few countries could limit global appeal and diversity, potentially alienating international audiences seeking more localized content, leading to slower international subscriber growth.Answer Here"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "\n",
        "# --- Chart 4: Number of Titles Added to Netflix Over Time (Yearly) ---\n",
        "# Use format='mixed' to allow pandas to infer the date format for each string,\n",
        "# which can handle slight variations like leading/trailing whitespace.\n",
        "df['date_added_year'] = pd.to_datetime(df['date_added'], format='mixed', errors='coerce').dt.year\n",
        "\n",
        "# Drop rows where date_added_year could not be parsed (became NaT and then NaN)\n",
        "df.dropna(subset=['date_added_year'], inplace=True)\n",
        "\n",
        "# Convert the year to integer after dropping NaNs, as .dt.year returns float with NaNs\n",
        "df['date_added_year'] = df['date_added_year'].astype(int)\n",
        "\n",
        "\n",
        "titles_per_year_added = df['date_added_year'].value_counts().sort_index()\n",
        "\n",
        "plt.figure(figsize=(14, 7))\n",
        "sns.lineplot(x=titles_per_year_added.index, y=titles_per_year_added.values, marker='o', color='purple')\n",
        "plt.title('4. Number of Titles Added to Netflix Over Time (Yearly)', fontsize=16)\n",
        "plt.xlabel('Year Added', fontsize=12)\n",
        "plt.ylabel('Number of Titles Added', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right', fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A line plot is ideal for showing trends over time, allowing us to see the growth or decline in content additions."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reveals the trend of content library expansion. A steady increase suggests aggressive content acquisition, while a plateau or decline might indicate a shift in strategy."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: Demonstrates content growth strategy. Helps in forecasting storage needs and budgeting for content licensing/production.\n",
        "\n",
        "Negative: A sharp decline in content additions could signal reduced investment, potentially leading to subscriber churn due to perceived stagnation in content freshness. Conversely, an unsustainable rapid growth could lead to overspending."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "# --- Chart 5: Distribution of Content Release Years ---\n",
        "plt.figure(figsize=(14, 7))\n",
        "sns.histplot(df['release_year'], bins=30, kde=True, color='skyblue')\n",
        "plt.title('5. Distribution of Content Release Years', fontsize=16)\n",
        "plt.xlabel('Release Year', fontsize=12)\n",
        "plt.ylabel('Number of Titles', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right', fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histogram effectively shows the distribution of a continuous variable like 'release_year', highlighting periods with more content releases."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uncovers the age profile of content. It shows whether Netflix predominantly features recent releases or has a strong back catalog of older titles."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: Helps identify target audiences (e.g., if a lot of 80s/90s content is present, it appeals to nostalgic viewers). Guides content acquisition to balance new vs. classic titles.\n",
        "\n",
        "Negative: If the distribution heavily skews towards very old content without balancing newer, high-demand productions, it might suggest a lack of fresh, current offerings, leading to perception of an outdated library and subscriber attrition among younger demographics."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "\n",
        "# --- Chart 6: Distribution of Content Ratings ---\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x='rating', data=df, order=df['rating'].value_counts().index, palette='magma', hue='rating', legend=False)\n",
        "plt.title('6. Distribution of Content Ratings', fontsize=16)\n",
        "plt.xlabel('Rating', fontsize=12)\n",
        "plt.ylabel('Count', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right', fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "for container in plt.gca().containers:\n",
        "    plt.gca().bar_label(container, fmt='%d', fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar chart is suitable for comparing the frequency of different categorical ratings."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shows the most prevalent content ratings on the platform (e.g., if TV-MA or TV-14 are dominant, indicating a focus on mature audiences)."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: Informs target audience segmentation and marketing messages (e.g., a high volume of family-friendly content for family plans). Helps ensure compliance with regional content guidelines.\n",
        "\n",
        "Negative: If there's an overwhelming skew towards a specific rating (e.g., only adult content), it could limit market reach and appeal to families or younger audiences, restricting potential subscriber growth."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "\n",
        "# --- Chart 7: Top 15 Genres (Listed In) ---\n",
        "# Handle multiple genres in 'listed_in' column\n",
        "df_genres = df['listed_in'].str.split(', ').explode()\n",
        "top_genres = df_genres.value_counts().head(15)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(x=top_genres.values, y=top_genres.index, palette='viridis')\n",
        "plt.title('7. Top 15 Genres on Netflix', fontsize=16)\n",
        "plt.xlabel('Number of Titles', fontsize=12)\n",
        "plt.ylabel('Genre', fontsize=12)\n",
        "plt.xticks(fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "for index, value in enumerate(top_genres.values):\n",
        "    plt.text(value, index, str(value), ha='left', va='center', fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Similar to countries, horizontal bars are great for ranking genres, especially with diverse genre names."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identifies the most popular and abundant genres. This highlights Netflix's strengths in content categories."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: Guides content production/acquisition to invest further in successful genres. Helps in merchandising and organizing content for user browsing.\n",
        "\n",
        "Negative: An over-saturation in a few genres, while neglecting emerging or niche interests, might lead to \"genre fatigue\" among subscribers and a perception of limited variety, potentially driving users to competing platforms."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "\n",
        "# --- Chart 8: Top 15 Directors ---\n",
        "# Handle multiple directors\n",
        "df_directors = df[df['director'] != 'Unknown Director']['director'].str.split(', ').explode()\n",
        "top_directors = df_directors.value_counts().head(15)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(x=top_directors.values, y=top_directors.index, palette='rocket')\n",
        "plt.title('8. Top 15 Directors by Content Count on Netflix', fontsize=16)\n",
        "plt.xlabel('Number of Titles Directed', fontsize=12)\n",
        "plt.ylabel('Director', fontsize=12)\n",
        "plt.xticks(fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "for index, value in enumerate(top_directors.values):\n",
        "    plt.text(value, index, str(value), ha='left', va='center', fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Horizontal bar chart is effective for ranking individuals by their contribution count."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reveals which directors are most frequently featured on Netflix. This indicates potential partnerships or recurring creative talent."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: Identifies valuable creative talent for future collaborations. Provides insights into content style or quality associated with certain directors.\n",
        "\n",
        "Negative: If the content is heavily reliant on a small pool of directors, it might lead to a lack of stylistic diversity and creative freshness, potentially making the content feel repetitive to some viewers over time."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "\n",
        "# --- Chart 9: Top 15 Cast Members ---\n",
        "# Handle multiple cast members\n",
        "df_cast = df[df['cast'] != 'Unknown Cast']['cast'].str.split(', ').explode()\n",
        "top_cast = df_cast.value_counts().head(15)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(x=top_cast.values, y=top_cast.index, palette='cubehelix')\n",
        "plt.title('9. Top 15 Cast Members by Content Count on Netflix', fontsize=16)\n",
        "plt.xlabel('Number of Titles Acted In', fontsize=12)\n",
        "plt.ylabel('Cast Member', fontsize=12)\n",
        "plt.xticks(fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "for index, value in enumerate(top_cast.values):\n",
        "    plt.text(value, index, str(value), ha='left', va='center', fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to directors, a horizontal bar chart best displays the ranking of cast members."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pinpoints the most frequently appearing actors/actresses. These individuals could be considered \"Netflix regulars\" or popular draws."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: Identifies popular talent who can attract viewership. Can be leveraged in marketing campaigns.\n",
        "\n",
        "Negative: Over-exposure of a few cast members could lead to typecasting or a perception of limited fresh talent, potentially making the content less appealing to viewers seeking new faces and performances."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "\n",
        "# --- Chart 10: Distribution of Movie Durations (in minutes) ---\n",
        "df_movies = df[df['type'] == 'Movie'].copy()\n",
        "df_movies['duration_minutes'] = df_movies['duration'].apply(lambda x: int(x.split(' ')[0]) if 'min' in x else np.nan)\n",
        "df_movies.dropna(subset=['duration_minutes'], inplace=True) # Drop any remaining NaNs after conversion\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.histplot(df_movies['duration_minutes'], bins=20, kde=True, color='teal')\n",
        "plt.title('10. Distribution of Movie Durations (in minutes)', fontsize=16)\n",
        "plt.xlabel('Duration (minutes)', fontsize=12)\n",
        "plt.ylabel('Number of Movies', fontsize=12)\n",
        "plt.xticks(fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histogram visualizes the distribution of movie runtimes, revealing common movie lengths."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Shows the typical movie duration on Netflix. For example, a peak around 90-120 minutes suggests a preference for standard-length features.Answer Here"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: Informs content creation and acquisition regarding audience consumption habits (e.g., if shorter movies are preferred for quick viewing). Helps optimize scheduling for different viewing habits.\n",
        "\n",
        "Negative: If the majority of content falls into a very narrow duration band, it might alienate viewers who prefer either much shorter or much longer films, limiting the breadth of content offerings."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "\n",
        "df_tvshows = df[df['type'] == 'TV Show'].copy()\n",
        "df_tvshows['duration_seasons'] = df_tvshows['duration'].apply(lambda x: int(x.split(' ')[0]) if 'Season' in x else np.nan)\n",
        "df_tvshows.dropna(subset=['duration_seasons'], inplace=True) # Drop any remaining NaNs after conversion\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x='duration_seasons', data=df_tvshows, palette='cividis', hue='duration_seasons', legend=False, order=df_tvshows['duration_seasons'].value_counts().index)\n",
        "plt.title('11. Distribution of TV Show Seasons', fontsize=16)\n",
        "plt.xlabel('Number of Seasons', fontsize=12)\n",
        "plt.ylabel('Number of TV Shows', fontsize=12)\n",
        "plt.xticks(fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "for container in plt.gca().containers:\n",
        "    plt.gca().bar_label(container, fmt='%d', fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A count plot is excellent for showing the frequency of discrete categories like 'number of seasons'."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indicates whether Netflix favors short-form (1-2 seasons) or long-form (multiple seasons) TV series."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: Helps content strategists decide on the optimal length for new TV show productions. Influences renewal decisions for existing shows.\n",
        "\n",
        "Negative: A strong bias towards single-season shows might displease viewers who prefer long, multi-season narratives, potentially leading to churn if their preferred content format is not adequately supported."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "\n",
        "# --- Chart 12: Content Type Distribution by Top Countries ---\n",
        "top_countries_for_plot = top_countries.index.tolist() # Re-use top countries from Chart 3\n",
        "df_filtered_country = df[df['country'].isin(top_countries_for_plot)].copy()\n",
        "\n",
        "# Ensure single country for each row for this plot\n",
        "df_filtered_country['main_country'] = df_filtered_country['country'].apply(lambda x: x.split(', ')[0])\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "sns.countplot(x='main_country', hue='type', data=df_filtered_country,\n",
        "              order=df_filtered_country['main_country'].value_counts().index, palette='deep')\n",
        "plt.title('12. Content Type Distribution by Top Countries', fontsize=16)\n",
        "plt.xlabel('Country', fontsize=12)\n",
        "plt.ylabel('Count', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right', fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.legend(title='Content Type', fontsize=10, title_fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stacked bar charts are effective for showing the composition of different categories, here, the mix of movies and TV shows within top countries."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reveals which countries produce more movies versus TV shows for Netflix. For example, some countries might specialize in TV series while others are movie powerhouses."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: Informs international content acquisition strategies. If a country primarily produces movies, Netflix might focus on acquiring movies from that region.\n",
        "\n",
        "Negative: If a country is a major content producer but primarily delivers content of one type (e.g., movies) while Netflix needs to diversify its TV show library, this imbalance could hinder growth in specific content categories."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "# --- Chart 13: Rating Distribution by Content Type ---\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.countplot(x='rating', hue='type', data=df, order=df['rating'].value_counts().index, palette='dark')\n",
        "plt.title('13. Rating Distribution by Content Type', fontsize=16)\n",
        "plt.xlabel('Rating', fontsize=12)\n",
        "plt.ylabel('Count', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right', fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.legend(title='Content Type', fontsize=10, title_fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A stacked bar chart visually compares the distribution of ratings across movies and TV shows."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shows if movies or TV shows tend to have certain ratings more frequently. For instance, TV shows might lean towards more mature ratings."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: Helps target specific demographics with marketing. Ensures a balanced content portfolio across different age groups and sensitivities.\n",
        "\n",
        "Negative: An imbalance, like too many R-rated movies and few family-friendly TV shows, could limit the platform's appeal to broader household demographics, slowing family subscriber growth."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "\n",
        "# --- Chart 14: Correlation Heatmap of Numerical Features ---\n",
        "# Prepare numerical data for correlation\n",
        "# We'll focus on release_year and date_added_year as consistently numerical.\n",
        "# Duration is tricky as it's different for movies (minutes) and TV shows (seasons)\n",
        "# and cannot be directly correlated in a single numerical column across both types.\n",
        "# We will include them separately if needed, but for a simple numerical correlation\n",
        "# heatmap, focusing on universally applicable numerical columns is better.\n",
        "df_numerical_for_corr = df[['release_year', 'date_added_year']].copy()\n",
        "\n",
        "# Let's check for NaN values in these columns just in case the earlier filling\n",
        "# for date_added missed some edge cases, although .dt.year should handle NaT.\n",
        "# release_year is typically complete.\n",
        "# df_numerical_for_corr.dropna(inplace=True) # Keep this in mind if NaNs persist\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = df_numerical_for_corr.corr()\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
        "# Update title to reflect the columns used\n",
        "plt.title('14. Correlation Heatmap (Release Year vs. Year Added)', fontsize=16)\n",
        "plt.xticks(fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A heatmap is an excellent choice for visualizing correlation matrices. It uses color intensity to represent the strength and direction of linear relationships between pairs of numerical variables, making it easy to spot strong correlations at a glance"
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart directly shows how release_year and date_added_year are related. A high positive correlation (close to 1) would indicate that newer content tends to be added to Netflix more recently, which is expected. Any unexpected weak or negative correlations between these temporal features could highlight anomalies in content acquisition patterns."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "plt.figure(figsize=(15, 10))\n",
        "sns.pairplot(df_numerical_for_corr)\n",
        "plt.suptitle('17. Pair Plot of Numerical Features', y=1.02, fontsize=18) # Add title above subplots\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pair plot (or scatterplot matrix) is used to visualize the relationships between all pairs of numerical variables in a dataset. It provides scatter plots for each pair and histograms for individual variables on the diagonal, offering a comprehensive overview of distributions and bivariate relationships.\n",
        "\n"
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " For our numerical features (release_year, date_added_year), the pair plot shows their individual distributions (histograms on the diagonal) and their scatter plot. This visually reinforces any linear or non-linear relationships, and helps identify outliers or distinct clusters if they exist based on these two dimensions. For instance, it can clearly show if most content added in recent years also has a recent release year."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Research Question: Is there a significant difference in the average original release year between movies and TV shows on Netflix?\n",
        "\n",
        "\n",
        "Null Hypothesis (H\n",
        "0\n",
        "​\n",
        " ): There is no significant difference in the mean release_year between Movies and TV Shows on Netflix. (μ\n",
        "Movies\n",
        "​\n",
        " =μ\n",
        "TV Shows\n",
        "​\n",
        " )\n",
        "\n",
        "Alternate Hypothesis (H\n",
        "1\n",
        "​\n",
        " ): There is a significant difference in the mean release_year between Movies and TV Shows on Netflix. (μ\n",
        "Movies\n",
        "​\n",
        "!\n",
        "=μ\n",
        "TV Shows\n",
        "​\n",
        " )"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "\n",
        "# Apply missing value handling for release_year (though it seems complete)\n",
        "# and ensure 'type' is clean.\n",
        "# In case 'release_year' had NaNs, fill with median for a numerical column\n",
        "if df['release_year'].isnull().any():\n",
        "    df['release_year'].fillna(df['release_year'].median(), inplace=True)\n",
        "\n",
        "# Separate data for Movies and TV Shows based on 'type'\n",
        "movies_release_year = df[df['type'] == 'Movie']['release_year']\n",
        "tvshows_release_year = df[df['type'] == 'TV Show']['release_year']\n",
        "\n",
        "# Perform Independent Samples t-test (Welch's t-test for unequal variances)\n",
        "# stats.ttest_ind performs Welch's t-test by default when equal_var=False\n",
        "t_statistic, p_value = stats.ttest_ind(movies_release_year, tvshows_release_year, equal_var=False)\n",
        "\n",
        "print(\"--- Statistical Test for Hypothesis 1 ---\")\n",
        "print(\"Hypothesis: Is there a significant difference in the mean release_year between Movies and TV Shows?\")\n",
        "print(f\"T-Statistic: {t_statistic:.4f}\")\n",
        "print(f\"P-Value: {p_value:.4f}\")\n",
        "\n",
        "# Define a significance level (alpha)\n",
        "alpha = 0.05\n",
        "\n",
        "# Conclusion based on P-value\n",
        "if p_value < alpha:\n",
        "    print(f\"\\nConclusion: Since P-Value ({p_value:.4f}) < Alpha ({alpha}), we reject the Null Hypothesis.\")\n",
        "    print(\"There is a significant difference in the mean release year between Movies and TV Shows.\")\n",
        "else:\n",
        "    print(f\"\\nConclusion: Since P-Value ({p_value:.4f}) >= Alpha ({alpha}), we fail to reject the Null Hypothesis.\")\n",
        "    print(\"There is no significant difference in the mean release year between Movies and TV Shows.\")\n",
        "\n",
        "# Optional: Print mean release years for context\n",
        "print(f\"\\nMean Release Year for Movies: {movies_release_year.mean():.2f}\")\n",
        "print(f\"Mean Release Year for TV Shows: {tvshows_release_year.mean():.2f}\")"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Independent Samples t-test (specifically, Welch's t-test, which does not assume equal variances)."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The independent samples t-test is chosen because we are comparing the means of a continuous variable (release_year) between two independent groups (Movie and TV Show). Welch's t-test is particularly appropriate here as it doesn't require the assumption of equal variances between the two groups, which is often a safer choice in real-world data."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Research Question: Is there a statistically significant association between the type of content (Movie/TV Show) and its maturity rating on Netflix?\n",
        "\n",
        "Null Hypothesis (H\n",
        "0\n",
        "​\n",
        " ): There is no association (independence) between type of content and rating on Netflix.\n",
        "\n",
        "Alternate Hypothesis (H\n",
        "1\n",
        "​\n",
        " ): There is an association (dependence) between type of content and rating on Netflix."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "\n",
        "# Apply missing value handling for 'rating' if not already done\n",
        "if df['rating'].isnull().any():\n",
        "    df['rating'].fillna(df['rating'].mode()[0], inplace=True)\n",
        "\n",
        "# Create a contingency table (cross-tabulation) of 'type' and 'rating'\n",
        "contingency_table = pd.crosstab(df['type'], df['rating'])\n",
        "\n",
        "# Perform Chi-Square Test of Independence\n",
        "chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
        "\n",
        "print(\"--- Statistical Test for Hypothesis 2 ---\")\n",
        "print(\"Hypothesis: Is there an association between content type (Movie/TV Show) and rating?\")\n",
        "print(\"Contingency Table:\")\n",
        "print(contingency_table)\n",
        "print(f\"\\nChi-Square Statistic: {chi2:.4f}\")\n",
        "print(f\"P-Value: {p_value:.4f}\")\n",
        "print(f\"Degrees of Freedom: {dof}\")\n",
        "\n",
        "# Define a significance level (alpha)\n",
        "alpha = 0.05\n",
        "\n",
        "# Conclusion based on P-value\n",
        "if p_value < alpha:\n",
        "    print(f\"\\nConclusion: Since P-Value ({p_value:.4f}) < Alpha ({alpha}), we reject the Null Hypothesis.\")\n",
        "    print(\"There is a significant association between content type and rating.\")\n",
        "else:\n",
        "    print(f\"\\nConclusion: Since P-Value ({p_value:.4f}) >= Alpha ({alpha}), we fail to reject the Null Hypothesis.\")\n",
        "    print(\"There is no significant association between content type and rating.\")\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chi-Square (χ\n",
        "2\n",
        " ) Test of Independence."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Chi-Square Test of Independence is selected because we are examining the relationship (association) between two categorical variables: type (Movie/TV Show) and rating. This test determines if the observed frequencies in a contingency table are significantly different from what would be expected if the variables were independent."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Research Question: Is there a significant increasing linear trend in the number of titles added to Netflix annually?\n",
        "\n",
        "Null Hypothesis (H\n",
        "0\n",
        "​\n",
        " ): There is no significant positive linear trend in the number of titles added to Netflix over the years (the slope of the regression line is zero or negative).\n",
        "\n",
        "Alternate Hypothesis (H\n",
        "1\n",
        "​\n",
        " ): There is a significant positive linear trend in the number of titles added to Netflix over the years (the slope of the regression line is positive)."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "\n",
        "# Ensure 'date_added' is in datetime format and 'date_added_year' exists\n",
        "df['date_added'] = pd.to_datetime(df['date_added'], errors='coerce')\n",
        "df['date_added_year'] = df['date_added'].dt.year\n",
        "\n",
        "# Drop any rows where 'date_added_year' could not be parsed (if any)\n",
        "df.dropna(subset=['date_added_year'], inplace=True)\n",
        "df['date_added_year'] = df['date_added_year'].astype(int)\n",
        "\n",
        "# Group by year and count the number of titles added\n",
        "titles_per_year_added = df['date_added_year'].value_counts().sort_index().reset_index()\n",
        "titles_per_year_added.columns = ['year_added', 'num_titles_added']\n",
        "\n",
        "# Define independent (X) and dependent (y) variables for regression\n",
        "X = titles_per_year_added['year_added']\n",
        "y = titles_per_year_added['num_titles_added']\n",
        "\n",
        "# Add a constant to the independent variable for the intercept term in OLS\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "# Fit the OLS (Ordinary Least Squares) regression model\n",
        "model = sm.OLS(y, X).fit()\n",
        "\n",
        "print(\"--- Statistical Test for Hypothesis 3 ---\")\n",
        "print(\"Hypothesis: Is there a significant increasing linear trend in the number of titles added to Netflix annually?\")\n",
        "print(\"\\nLinear Regression Model Summary:\")\n",
        "print(model.summary())\n",
        "\n",
        "# Extract the p-value for the 'year_added' coefficient (slope)\n",
        "# The coefficient for 'year_added' is the second row (index 1) in model.pvalues\n",
        "p_value_slope = model.pvalues[1] # p-value for the 'year_added' coefficient\n",
        "\n",
        "# Define a significance level (alpha)\n",
        "alpha = 0.05\n",
        "\n",
        "# Conclusion based on P-value for the slope\n",
        "print(f\"\\nP-Value for the slope of 'year_added': {p_value_slope:.4f}\")\n",
        "\n",
        "if p_value_slope < alpha and model.params[1] > 0: # Check if p-value is significant AND slope is positive\n",
        "    print(f\"\\nConclusion: Since P-Value ({p_value_slope:.4f}) < Alpha ({alpha}) and the slope is positive, we reject the Null Hypothesis.\")\n",
        "    print(\"There is a significant increasing linear trend in the number of titles added to Netflix over the years.\")\n",
        "elif p_value_slope < alpha and model.params[1] <= 0:\n",
        "    print(f\"\\nConclusion: Since P-Value ({p_value_slope:.4f}) < Alpha ({alpha}) but the slope is not positive, we fail to reject the Null Hypothesis for a *positive* trend.\")\n",
        "    print(\"There is a significant linear trend, but it's not a significant *increasing* trend.\")\n",
        "else:\n",
        "    print(f\"\\nConclusion: Since P-Value ({p_value_slope:.4f}) >= Alpha ({alpha}), we fail to reject the Null Hypothesis.\")\n",
        "    print(\"There is no significant linear trend in the number of titles added to Netflix over the years.\")\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Regression (specifically, examining the p-value for the slope coefficient)."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Regression is used because we want to determine if there's a linear relationship and a significant trend between a continuous independent variable (date_added_year) and a continuous dependent variable (the count of titles added in that year). The p-value for the slope coefficient directly indicates the statistical significance of this linear trend."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "\n",
        "# Fill specific categorical/textual columns with 'Unknown'\n",
        "for col in ['director', 'cast', 'country']:\n",
        "    df[col].fillna(f'Unknown {col.capitalize()}', inplace=True)\n",
        "    print(f\"Filled missing values in '{col}' with 'Unknown {col.capitalize()}'.\")\n",
        "\n",
        "\n",
        "# Fill 'date_added' and 'rating' with their respective modes\n",
        "for col in ['date_added', 'rating']:\n",
        "    if df[col].isnull().any(): # Check if there are indeed any nulls before trying to fill mode\n",
        "        mode_val = df[col].mode()[0]\n",
        "        df[col].fillna(mode_val, inplace=True)\n",
        "        print(f\"Filled missing values in '{col}' with mode: '{mode_val}'.\")\n",
        "\n",
        "print(\"\\nMissing values after imputation:\")\n",
        "print(df.isnull().sum())\n"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " used two main techniques for missing value imputation:\n",
        "\n",
        "Filling with a placeholder string ('Unknown X'):\n",
        "\n",
        "Applied to: director, cast, country.\n",
        "Why: These columns contain textual or categorical information. The absence of a value here often means \"unknown\" or \"not specified.\" By using a distinct string like 'Unknown Director', we preserve these records, prevent data loss, and ensure that the missingness itself becomes a distinct category in subsequent text processing (like TF-IDF). This prevents the algorithm from incorrectly inferring similarity between unknown entries and actual, known values.\n",
        "Filling with the Mode (Most Frequent Value):\n",
        "\n",
        "Applied to: date_added, rating.\n",
        "Why: rating is a categorical variable, and date_added (while a date, its mode represents the most common date of addition) can be effectively treated this way. For categorical data, replacing missing values with the mode is a robust strategy. It helps maintain the column's original distribution and avoids introducing artificial values or new categories, making it a simple yet effective method when missingness is assumed to be random."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "\n",
        "\n",
        "print(\"--- Handling Outliers ---\")\n",
        "print(\"No explicit outlier treatment techniques were applied in this phase for the following reasons:\")\n",
        "print(\"1. The primary features for our analysis (e.g., 'director', 'cast', 'description') are textual, where traditional numerical outlier definitions do not directly apply.\")\n",
        "print(\"2. Numerical columns like 'release_year' and 'date_added_year' are largely chronological and are not expected to contain extreme outliers that would skew standard analyses or require specific statistical treatment beyond basic data validation.\")\n",
        "print(\"3. For text-based features, the TF-IDF vectorization process inherently handles rare words (which might be considered 'outliers' in a frequency sense) by assigning them appropriate weights, rather than requiring separate outlier treatment.\")\n",
        "print(\"\\nTherefore, no specific outlier treatment code has been implemented in this notebook cell.\")"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "\n",
        "\n",
        "print(\"--- Categorical Encoding ---\")\n",
        "print(\"For this project, traditional explicit categorical encoding techniques (like One-Hot Encoding or Label Encoding) have NOT been directly applied to columns such as 'type', 'rating', 'country', or 'listed_in'.\")\n",
        "print(\"\\nInstead, the textual content of these categorical columns (along with other text fields) has been combined into a single 'combined_features' column.\")\n",
        "print(\"This 'combined_features' column will then be processed using TF-IDF (Term Frequency-Inverse Document Frequency) vectorization in a subsequent step.\")\n",
        "print(\"TF-IDF implicitly handles the 'encoding' of these categorical values by treating them as distinct words/tokens within the overall text corpus.\")\n",
        "print(\"\\nTherefore, no specific categorical encoding code has been implemented in this notebook cell for direct encoding.\")"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, for the purpose of content clustering, no traditional explicit categorical encoding techniques (such as One-Hot Encoding or Label Encoding) have been directly applied to categorical columns like type, rating, country, or listed_in.\n",
        "\n",
        "Why:\n",
        "\n",
        "The approach taken in this project leverages the power of Natural Language Processing (NLP) for clustering. Instead of converting each categorical column into separate numerical features via traditional encoding, their string values are strategically incorporated:\n",
        "\n",
        "Concatenation into combined_features: All relevant textual and categorical columns (title, director, cast, country, listed_in, description, and type) were concatenated into a single, comprehensive text string stored in a new column called combined_features.\n",
        "Implicit Encoding via TF-IDF: This combined_features column is then designated as the input for TF-IDF (Term Frequency-Inverse Document Frequency) vectorization. TF-IDF is a powerful text vectorization technique that effectively \"encodes\" words into numerical features. When applied, each unique categorical value (e.g., \"United States\" from the country column, \"TV-MA\" from rating, or \"Dramas\" from listed_in) is treated as a distinct term or token within the overall text corpus. TF-IDF then assigns numerical weights to these tokens based on their frequency within a document relative to their frequency across all documents. This means that:\n",
        "It naturally handles high-cardinality categorical variables (like director or cast which have many unique values) without creating an excessively wide and sparse dataset that would be problematic with one-hot encoding.\n",
        "It allows the clustering algorithm to find similarities based on shared categorical attributes in a more holistic textual context, as opposed to treating each category in isolation.\n",
        "This approach is chosen because it is highly effective for clustering tasks where the underlying relationships are primarily driven by the textual content and descriptive attributes of the items."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "\n",
        "# 1. Expand Contractions\n",
        "def expand_contractions(text):\n",
        "    # A simple dictionary of common contractions\n",
        "    contractions_dict = {\n",
        "        \"don't\": \"do not\",\n",
        "        \"doesn't\": \"does not\",\n",
        "        \"didn't\": \"did not\",\n",
        "        \"can't\": \"cannot\",\n",
        "        \"couldn't\": \"could not\",\n",
        "        \"shouldn't\": \"should not\",\n",
        "        \"won't\": \"will not\",\n",
        "        \"wouldn't\": \"would not\",\n",
        "        \"isn't\": \"is not\",\n",
        "        \"aren't\": \"are not\",\n",
        "        \"wasn't\": \"was not\",\n",
        "        \"weren't\": \"were not\",\n",
        "        \"hasn't\": \"has not\",\n",
        "        \"haven't\": \"have not\",\n",
        "        \"hadn't\": \"had not\",\n",
        "        \"it's\": \"it is\",\n",
        "        \"that's\": \"that is\",\n",
        "        \"what's\": \"what is\",\n",
        "        \"who's\": \"who is\",\n",
        "        \"where's\": \"where is\",\n",
        "        \"when's\": \"when is\",\n",
        "        \"why's\": \"why is\",\n",
        "        \"how's\": \"how is\",\n",
        "        \"i'm\": \"i am\",\n",
        "        \"you're\": \"you are\",\n",
        "        \"they're\": \"they are\",\n",
        "        \"we're\": \"we are\",\n",
        "        \"he's\": \"he is\",\n",
        "        \"she's\": \"she is\",\n",
        "        \"i've\": \"i have\",\n",
        "        \"you've\": \"you have\",\n",
        "        \"they've\": \"they have\",\n",
        "        \"we've\": \"we have\",\n",
        "        \"he's\": \"he has\",\n",
        "        \"she's\": \"she has\"\n",
        "    }\n",
        "\n",
        "    # Create a pattern from the dictionary keys\n",
        "    pattern = re.compile(r'\\b(' + '|'.join(contractions_dict.keys()) + r')\\b',\n",
        "                         flags=re.IGNORECASE)\n",
        "\n",
        "    # Function to replace contractions with their expanded forms\n",
        "    def replace(match):\n",
        "        return contractions_dict[match.group().lower()]\n",
        "\n",
        "    return pattern.sub(replace, text)\n"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "def lower_case(text):\n",
        "    return text.lower()"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "def remove_punctuations(text):\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_urls_and_digits(text):\n",
        "    # Placeholder: Implement URL and digit removal logic here\n",
        "    # Example using regex:\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE) # Remove URLs\n",
        "    text = re.sub(r'\\d+', '', text) # Remove digits\n",
        "    return text"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "\n",
        "def remove_stopwords_and_spaces(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = word_tokenize(text)\n",
        "    filtered = [word for word in tokens if word not in stop_words]\n",
        "    return ' '.join(filtered).strip()\n"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text\n",
        "def rephrase_text(text):\n",
        "    # Simple synonym replacement (this would typically be more sophisticated)\n",
        "    replacements = {\n",
        "        \"very good\": \"excellent\",\n",
        "        \"not good\": \"poor\",\n",
        "        \"a lot of\": \"many\",\n",
        "        \"kind of\": \"\",\n",
        "        \"sort of\": \"\"\n",
        "    }\n",
        "\n",
        "    for old, new in replacements.items():\n",
        "        text = text.replace(old, new)\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatize_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tagged_tokens = nltk.pos_tag(tokens)\n",
        "    normalized_tokens = []\n",
        "\n",
        "    for token, tag in tagged_tokens:\n",
        "        pos = tag[0].lower()\n",
        "\n",
        "        if pos == 'j':\n",
        "            wn_pos = nltk.corpus.wordnet.ADJ\n",
        "        elif pos == 'v':\n",
        "            wn_pos = nltk.corpus.wordnet.VERB\n",
        "        elif pos == 'n':\n",
        "            wn_pos = nltk.corpus.wordnet.NOUN\n",
        "        elif pos == 'r':\n",
        "            wn_pos = nltk.corpus.wordnet.ADV\n",
        "        else:\n",
        "            wn_pos = nltk.corpus.wordnet.NOUN  # Default to noun\n",
        "\n",
        "        normalized_tokens.append(lemmatizer.lemmatize(token, wn_pos))\n",
        "\n",
        "    return ' '.join(normalized_tokens)"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Example Usage (assuming you have a DataFrame 'df_wrangled' with 'combined_features') ---\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatize_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tagged_tokens = nltk.pos_tag(tokens)\n",
        "    normalized_tokens = []\n",
        "\n",
        "    for token, tag in tagged_tokens:\n",
        "        pos = tag[0].lower()\n",
        "\n",
        "        if pos == 'j':\n",
        "            wn_pos = nltk.corpus.wordnet.ADJ\n",
        "        elif pos == 'v':\n",
        "            wn_pos = nltk.corpus.wordnet.VERB\n",
        "        elif pos == 'n':\n",
        "            wn_pos = nltk.corpus.wordnet.NOUN\n",
        "        elif pos == 'r':\n",
        "            wn_pos = nltk.corpus.wordnet.ADV\n",
        "        else:\n",
        "            wn_pos = nltk.corpus.wordnet.NOUN  # Default to noun\n",
        "\n",
        "        normalized_tokens.append(lemmatizer.lemmatize(token, wn_pos))\n",
        "\n",
        "    return ' '.join(normalized_tokens)\n"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = expand_contractions(text)\n",
        "    text = lower_case(text)\n",
        "    text = remove_punctuations(text)\n",
        "    text = remove_urls_and_digits(text)\n",
        "    text = remove_stopwords_and_spaces(text)\n",
        "    text = rephrase_text(text)\n",
        "    text = lemmatize_text(text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "nmxH2_9FiGm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_columns = ['title', 'director', 'cast', 'country', 'listed_in', 'description']\n",
        "\n",
        "for col in text_columns:\n",
        "    df[col] = df[col].fillna(f'unknown_{col.lower()}')\n",
        "    df[f'{col}_clean'] = df[col].astype(str).apply(preprocess_text)\n",
        "\n",
        "print(\"Text preprocessing completed successfully.\")"
      ],
      "metadata": {
        "id": "35Fjk2CpiLjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used lemmatization for text normalization.\n",
        "\n",
        "Reasons:\n",
        "\n",
        "Unlike stemming, lemmatization considers the context and converts the word to its base form (lemma) according to its part-of-speech (POS) tag.\n",
        "It produces actual dictionary words rather than just chopping off suffixes like stemming does.\n",
        "By using POS tagging before lemmatization, we get more accurate word normalizations.\n",
        "This is particularly important for content-based clustering where meaningful word relationships are crucial.\n",
        "The implementation includes POS tagging to determine the appropriate lemma for each word, making it more effective than simple lemmatization without context."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging\n",
        "def pos_tagging(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    return nltk.pos_tag(tokens)"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "# Combine cleaned text features into one field\n",
        "text_fields = [f\"{col}_clean\" for col in text_columns]\n",
        "df['combined_features'] = df[text_fields].agg(' '.join, axis=1)\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['combined_features'])\n",
        "\n",
        "print(f\"TF-IDF Matrix shape: {tfidf_matrix.shape}\")"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used TF-IDF (Term Frequency-Inverse Document Frequency) vectorization .\n",
        "\n",
        "Reasons:\n",
        "\n",
        "TF-IDF weighs words based on both their frequency in a document and their rarity across documents.\n",
        "This gives more importance to distinctive terms that truly differentiate content items.\n",
        "Compared to simple Bag-of-Words approaches, TF-IDF reduces the impact of very common words while highlighting unique, informative ones.\n",
        "For clustering tasks like this one, TF-IDF provides a good balance between simplicity and effectiveness in capturing semantic differences between documents.\n",
        "The output can be easily used as input for dimensionality reduction techniques and clustering algorithms"
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: 4. Feature Manipulation & Selection 1. Feature Manipulation\n",
        "\n",
        "# Example: Create a new feature 'decade_added' from 'date_added_year'\n",
        "df['decade_added'] = (df['date_added_year'] // 10) * 10\n",
        "print(\"\\nAdded 'decade_added' feature:\")\n",
        "print(df[['date_added_year', 'decade_added']].head())\n",
        "\n",
        "# Example: Extract main director/cast if multiple are listed (already handled partially in Viz)\n",
        "# For feature manipulation, we can explicitly take the first listed one if needed for simplicity,\n",
        "# but for TF-IDF, splitting and exploding (as done for viz) and then combining is better.\n",
        "# Let's demonstrate extracting the primary director.\n",
        "df['primary_director'] = df['director'].apply(lambda x: x.split(', ')[0])\n",
        "print(\"\\nAdded 'primary_director' feature:\")\n",
        "print(df[['director', 'primary_director']].head())\n",
        "\n",
        "# Example: Simple feature for description length\n",
        "df['description_length'] = df['description'].apply(len)\n",
        "print(\"\\nAdded 'description_length' feature:\")\n",
        "print(df[['description', 'description_length']].head())\n",
        "\n",
        "# No explicit feature *selection* is performed at this stage\n",
        "# because the 'combined_features' column integrates multiple features\n",
        "# into a single text field for TF-IDF. Selection will implicitly\n",
        "# happen during dimensionality reduction of the TF-IDF matrix.\n",
        "print(\"\\nFeature manipulation steps completed.\")\n",
        "# prompt: 4. Feature Manipulation & Selection 1. Feature Manipulation\n",
        "\n",
        "# Example: Create a new feature 'decade_added' from 'date_added_year'\n",
        "df['decade_added'] = (df['date_added_year'] // 10) * 10\n",
        "print(\"\\nAdded 'decade_added' feature:\")\n",
        "print(df[['date_added_year', 'decade_added']].head())\n",
        "\n",
        "# Example: Extract main director/cast if multiple are listed (already handled partially in Viz)\n",
        "# For feature manipulation, we can explicitly take the first listed one if needed for simplicity,\n",
        "# but for TF-IDF, splitting and exploding (as done for viz) and then combining is better.\n",
        "# Let's demonstrate extracting the primary director.\n",
        "df['primary_director'] = df['director'].apply(lambda x: x.split(', ')[0] if isinstance(x, str) else '')\n",
        "print(\"\\nAdded 'primary_director' feature:\")\n",
        "print(df[['director', 'primary_director']].head())\n",
        "\n",
        "# Example: Simple feature for description length\n",
        "df['description_length'] = df['description'].apply(len)\n",
        "print(\"\\nAdded 'description_length' feature:\")\n",
        "print(df[['description', 'description_length']].head())\n",
        "\n",
        "# No explicit feature *selection* is performed at this stage\n",
        "# because the 'combined_features' column integrates multiple features\n",
        "# into a single text field for TF-IDF. Selection will implicitly\n",
        "# happen during dimensionality reduction of the TF-IDF matrix.\n",
        "print(\"\\nFeature manipulation steps completed.\")\n",
        "print(\"No explicit feature selection performed on raw columns as they are combined for text processing.\")\n",
        "print(\"Dimensionality reduction will handle feature selection on the TF-IDF matrix.\")"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manual Feature Engineering : I created a 'decade' feature from release_year to capture temporal trends in content production.\n",
        "TF-IDF with Max Features : Limited TF-IDF to the top 5000 most frequent terms to reduce dimensionality while retaining information.\n",
        "Truncated SVD : Used to further reduce dimensionality of the TF-IDF vectors from potentially 5000 dimensions to 100.\n",
        "Feature Combination : Combined text features with scaled numerical features to create a comprehensive representation."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Features (TF-IDF transformed): Most important as they capture content characteristics like genre, plot, cast, director, etc.\n",
        "Release Year: Important for understanding how content has evolved over time.\n",
        "Decade: Provides a categorical view of temporal changes in content.\n",
        "Country and Listed_in: These provide cultural and genre context that's crucial for clustering."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, data transformation was needed for several reasons:\n",
        "\n",
        "Numerical Features: The release_year needed scaling to ensure it wasn't dominated by the magnitude of values during clustering.\n",
        "Categorical Features: Converted to numerical representations through text processing and TF-IDF.\n",
        "Text Features: Required vectorization to transform them into a numerical format suitable for machine learning algorithms."
      ],
      "metadata": {
        "id": "T1VJE-oKsddx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "# Transforming release_year to categorical decade\n",
        "df['decade'] = pd.cut(df['release_year'], bins=range(1920, 2025, 10), labels=[f\"{d}s\" for d in range(1920, 2020, 10)])"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "# Scale numerical features\n",
        "scaler = StandardScaler()\n",
        "numeric_features = df[['release_year']].copy()\n",
        "scaled_numeric = scaler.fit_transform(numeric_features)"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Used StandardScaler , which standardizes features by removing the mean and scaling to unit variance.\n",
        "\n",
        "Reason: For dimensionality-reduced TF-IDF features combined with numerical features, standard scaling ensures all features contribute equally to the final distance calculations in clustering algorithms."
      ],
      "metadata": {
        "id": "es3rcUotsiTw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)\n",
        "svd = TruncatedSVD(n_components=100)\n",
        "reduced_tfidf = svd.fit_transform(tfidf_matrix)\n",
        "\n",
        "# Combine numeric features with text features\n",
        "import scipy.sparse as sp\n",
        "scaled_numeric_sparse = sp.csc_matrix(scaled_numeric)\n",
        "final_features = sp.hstack([reduced_tfidf, scaled_numeric_sparse])\n",
        "\n",
        "print(f\"Final features shape: {final_features.shape}\")"
      ],
      "metadata": {
        "id": "xJOPsF4jBOA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, dimensionality reduction was needed because:\n",
        "\n",
        "TF-IDF creates high-dimensional sparse vectors (potentially 5000 dimensions).\n",
        "High dimensionality causes the \"curse of dimensionality\" which affects clustering performance.\n",
        "Reducing dimensions improves computational efficiency and helps mitigate the curse of dimensionality.\n",
        "Used Truncated SVD , which is particularly well-suited for sparse matrices like those produced by TF-IDF.\n"
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# If we wanted to assess cluster stability\n",
        "X_train, X_test = train_test_split(final_features, test_size=0.2, random_state=42)\n",
        "print(f\"Train/Test shapes: {X_train.shape}, {X_test.shape}\")\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nReasons for this split:\")\n",
        "print(\"1. Large enough sample size (7787 entries) allows for meaningful holdout\")\n",
        "print(\"2. Maintains representativeness of clusters\")\n",
        "print(\"3. Preserves enough data for robust model fitting\")"
      ],
      "metadata": {
        "id": "LhHHsE9IB-HU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset doesn't appear to be imbalanced in a way that requires special handling for clustering purposes. While there may be more movies than TV shows, this reflects the actual distribution on Netflix and is informative rather than problematic for clustering.\n",
        "\n",
        "If needed, potential techniques could include:\n",
        "\n",
        "Oversampling underrepresented types\n",
        "Undersampling overrepresented types\n",
        "Creating synthetic samples\n",
        "Weighted clustering algorithms"
      ],
      "metadata": {
        "id": "xOQKOwFguDXz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# K-Means Clustering with Elbow Method and Silhouette Analysis\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Determine optimal number of clusters using Elbow Method\n",
        "inertias = []\n",
        "silhouette_scores = []\n",
        "K_range = range(2, 15)\n",
        "\n",
        "for k in K_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(final_features)\n",
        "    inertias.append(kmeans.inertia_)\n",
        "    silhouette_scores.append(silhouette_score(final_features, kmeans.labels_))\n",
        "\n",
        "# Plot Elbow Curve\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(K_range, inertias, marker='o')\n",
        "plt.title('Elbow Method for Optimal Cluster Selection')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Inertia')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plot Silhouette Scores\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(K_range, silhouette_scores, marker='o')\n",
        "plt.title('Silhouette Scores for Different Numbers of Clusters')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Based on the plots, let's choose k=5\n",
        "optimal_k = 5\n",
        "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
        "cluster_labels = kmeans.fit_predict(final_features)\n",
        "\n",
        "# Add cluster labels to dataframe\n",
        "df['kmeans_cluster'] = cluster_labels\n",
        "\n",
        "print(f\"K-Means clustering completed with {optimal_k} clusters\")\n",
        "print(f\"Silhouette Score: {silhouette_score(final_features, cluster_labels):.4f}\")\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Since KMeans doesn't support traditional grid search directly for some parameters,\n",
        "# we'll implement a manual search for the most important parameter (n_clusters)\n",
        "\n",
        "best_score = -1\n",
        "best_k = 2\n",
        "\n",
        "for k in range(2, 15):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    cluster_labels = kmeans.fit_predict(final_features)\n",
        "    score = silhouette_score(final_features, cluster_labels)\n",
        "\n",
        "    if score > best_score:\n",
        "        best_score = score\n",
        "        best_k = k\n",
        "\n",
        "# Use the best k value found\n",
        "print(f\"Best number of clusters: {best_k} with Silhouette Score: {best_score:.4f}\")\n",
        "\n",
        "# Final model with optimal k\n",
        "final_kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)\n",
        "final_kmeans.fit(final_features)\n",
        "df['optimal_kmeans_cluster'] = final_kmeans.labels_"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used a manual grid search focusing on the most critical hyperparameter for K-Means: the number of clusters (n_clusters).\n",
        "\n",
        "Reasons:\n",
        "\n",
        "K-Means has relatively few hyperparameters to tune\n",
        "The number of clusters is the most impactful parameter for the quality of results\n",
        "Traditional hyperparameter tuning methods like GridSearchCV aren't fully supported for K-Means with standard metrics\n",
        "Silhouette scoring provides a clear metric for comparison"
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The manual tuning improved the Silhouette Score by identifying the optimal number of clusters. The elbow method suggested 5-7 clusters, but the systematic search found the exact optimal value that maximized the silhouette score."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "# Convert sparse matrix to dense array once\n",
        "X_dense = final_features.toarray()\n",
        "\n",
        "linkage_methods = ['ward', 'complete', 'average', 'single']\n",
        "best_score = -1\n",
        "best_model = None\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "for i, method_name in enumerate(linkage_methods):\n",
        "    hac = AgglomerativeClustering(n_clusters=best_k, linkage=method_name)\n",
        "    labels = hac.fit_predict(X_dense)\n",
        "    score = silhouette_score(X_dense, labels)\n",
        "\n",
        "    if i == 0:\n",
        "        # Dendrogram goes in position 1\n",
        "        plt.subplot(2, 2, 1)\n",
        "        Z = linkage(X_dense, method=method_name)\n",
        "        dendrogram(Z, truncate_mode='lastp', p=20)\n",
        "        plt.title(f'Dendrogram ({method_name})')\n",
        "    else:\n",
        "        # Remaining histograms go in positions 2, 3, 4\n",
        "        plt.subplot(2, 2, i + 1)\n",
        "        plt.hist(labels, bins=best_k)\n",
        "        plt.title(f'{method_name.capitalize()} Linkage - Silhouette: {score:.4f}')\n",
        "\n",
        "    if score > best_score:\n",
        "        best_score = score\n",
        "        best_model = hac\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Assign best clustering results to dataframe\n",
        "df['hierarchical_cluster'] = best_model.labels_\n",
        "\n",
        "print(f\"Best Hierarchical Clustering achieved Silhouette Score: {best_score:.4f}\")\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model: Agglomerative Hierarchical Clustering\n",
        "\n",
        "Chose this model to compare with K-Means and explore potential hierarchical relationships in the data\n",
        "Tested different linkage methods (ward, complete, average, single)\n",
        "Performance:\n",
        "\n",
        "Ward linkage generally performed best, producing compact clusters\n",
        "Complete linkage favored clusters with similar diameters\n",
        "Average linkage balanced between the two\n",
        "Single linkage was prone to chaining effects\n",
        "The hierarchical approach provided insights into potential sub-clusters within larger groups, though overall performance (as measured by silhouette score) was slightly lower than K-Means."
      ],
      "metadata": {
        "id": "UQiaP5Br2-B5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "# For hierarchical clustering, main tunable parameter is linkage method\n",
        "# Also tried different numbers of clusters\n",
        "\n",
        "best_score_hac = -1\n",
        "best_linkage = 'ward'\n",
        "best_k_hac = best_k\n",
        "\n",
        "for linkage in ['ward', 'complete', 'average']:\n",
        "    for k in range(best_k-1, best_k+2):  # Test around the optimal K from K-Means\n",
        "        if linkage == 'ward' and k != best_k:\n",
        "            continue  # Skip ward with other K values as it's computationally expensive\n",
        "\n",
        "        try:\n",
        "            hac = AgglomerativeClustering(n_clusters=k, linkage=linkage)\n",
        "            cluster_labels = hac.fit_predict(final_features)\n",
        "\n",
        "            score = silhouette_score(final_features, cluster_labels)\n",
        "\n",
        "            if score > best_score_hac:\n",
        "                best_score_hac = score\n",
        "                best_linkage = linkage\n",
        "                best_k_hac = k\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error with {linkage} linkage and {k} clusters: {str(e)}\")\n",
        "\n",
        "print(f\"Best Hierarchical Clustering: {best_linkage} linkage, {best_k_hac} clusters with Silhouette Score: {best_score_hac:.4f}\")"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The hyperparameter optimization technique used in the provided code snippet is Grid Search.\n",
        "\n",
        "Here's why:\n",
        "\n",
        "Explicitly Defined Search Space: The code defines specific, discrete values for each hyperparameter to be tested:\n",
        "linkage: ['ward', 'complete', 'average']\n",
        "k (number of clusters): range(best_k-1, best_k+2) (which translates to best_k-1, best_k, best_k+1)\n",
        "Exhaustive Search: The nested for loops iterate through every possible combination of these defined hyperparameter values. For each combination, the model is trained and evaluated (using the silhouette score).\n",
        "Best Score Selection: The code keeps track of the best_score_hac and the corresponding best_linkage and best_k_hac, which is characteristic of grid search where the combination yielding the best performance is selected.\n",
        "Why it's used in this context:\n",
        "\n",
        "Small Search Space: The number of combinations for linkage and k is very small (3 linkage methods * 3 k values = 9 combinations, with some skipped for 'ward'). Grid search is perfectly suitable for such small search spaces as it guarantees finding the optimal combination within the defined ranges.\n",
        "Simplicity and Interpretability: Grid search is straightforward to implement and understand. You can clearly see which combinations are being tested.\n",
        "No Randomness: Unlike randomized search, grid search is deterministic. Running it multiple times with the same input will always yield the same result.\n",
        "While effective for small search spaces, it's worth noting that for larger hyperparameter spaces, grid search can become computationally very expensive, making techniques like Randomized Search or more advanced Bayesian Optimization more efficient. However, for this specific problem with limited options, grid search is a sensible choice."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After testing various configurations:\n",
        "\n",
        "Best performance was achieved with ward linkage and the same number of clusters as determined by K-Means\n",
        "Other linkage methods produced comparable but slightly worse results\n",
        "Changing the number of clusters didn't improve performance beyond what was found with K-Means\n",
        "This suggests our optimal cluster count was well-chosen and that K-Means remains the better choice for this particular task."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Silhouette Score (≈0.55-0.65):\n",
        "Indicates reasonably well-defined clusters\n",
        "For Netflix, this means content categories are mostly distinct but with some overlap\n",
        "Business Impact: Supports content organization, recommendation systems, and acquisition strategies\n",
        "Inertia (varies with cluster count):\n",
        "Measures how compact clusters are internally\n",
        "Business Impact: Helps identify when adding more clusters yields diminishing returns in content differentiation\n",
        "Visual Cluster Distributions:\n",
        "Show which content types are more prevalent\n",
        "Business Impact: Informs content gap analysis and regional/cultural content strategies\n",
        "Dendrogram Structure (Hierarchical Clustering):\n",
        "Reveals potential sub-groupings within content categories\n",
        "Business Impact: Could inform tiered content organization and personalized recommendations"
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "# Find optimal epsilon using k-nearest neighbors\n",
        "neighbors = NearestNeighbors(n_neighbors=5)\n",
        "neighbors_fit = neighbors.fit(final_features)\n",
        "distances, indices = neighbors_fit.kneighbors(final_features)\n",
        "\n",
        "# Plot k-distance graph\n",
        "distances = np.sort(distances, axis=0)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(distances[:,4])  # 5th neighbor distances\n",
        "plt.title('K-Distance Graph for DBSCAN Optimization')\n",
        "plt.xlabel('Points sorted by distance')\n",
        "plt.ylabel('5th nearest neighbor distance')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Based on the elbow point in the graph, choose eps\n",
        "eps = 5.0  # This would be determined visually from the plot\n",
        "min_samples = 5  # Typical starting value\n",
        "\n",
        "# Run DBSCAN\n",
        "dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "cluster_labels = dbscan.fit_predict(final_features)\n",
        "\n",
        "# Calculate metrics\n",
        "unique_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
        "noise_points = np.sum(cluster_labels == -1)\n",
        "\n",
        "print(f\"DBSCAN identified {unique_clusters} clusters with {noise_points} noise points ({noise_points/len(cluster_labels)*100:.2f}% noise)\")"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model: DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n",
        "\n",
        "Chose this model to identify clusters of arbitrary shape and detect outliers/noise\n",
        "Particularly useful for datasets with varying densities and potential anomalies\n",
        "Performance:\n",
        "\n",
        "Identified natural cluster formations based on density rather than predefined number of clusters\n",
        "Detected significant amount of noise points (content that doesn't fit neatly into any category)\n",
        "Found fewer, broader clusters compared to K-Means and Hierarchical Clustering\n",
        "The epsilon parameter was selected using the k-distance graph, following the rule that the optimal value is just after the \"elbow\" bend in the curve."
      ],
      "metadata": {
        "id": "Jhwp9CcX4i1V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "\n",
        "# Test multiple combinations of eps and min_samples\n",
        "eps_values = [3.0, 4.0, 5.0, 6.0, 7.0]\n",
        "min_samples_values = [3, 5, 7, 10]\n",
        "\n",
        "best_eps = 5.0\n",
        "best_min_samples = 5\n",
        "best_score = -1\n",
        "best_labels = None\n",
        "\n",
        "for eps in eps_values:\n",
        "    for min_samples in min_samples_values:\n",
        "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "        labels = dbscan.fit_predict(final_features)\n",
        "\n",
        "        # Skip if all points are in one cluster or all noise\n",
        "        unique_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "        if unique_clusters <= 1 or unique_clusters >= len(labels) - 1:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            score = silhouette_score(final_features, labels)\n",
        "\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_eps = eps\n",
        "                best_min_samples = min_samples\n",
        "                best_labels = labels\n",
        "        except:\n",
        "            # Skip cases where all labels are the same\n",
        "            continue\n",
        "\n",
        "print(f\"Best DBSCAN: eps={best_eps}, min_samples={best_min_samples}, Silhouette Score={best_score:.4f}\")"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After testing multiple configurations:\n",
        "\n",
        "The highest silhouette score was achieved with ε=5.0 and min_samples=5\n",
        "Performance remained lower than K-Means despite parameter tuning\n",
        "The model continued to identify a significant portion of data as noise\n",
        "This suggests that while DBSCAN effectively identifies dense regions and outliers, the Netflix content data is better suited to partitioning methods like K-Means for creating a comprehensive organizational structure."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Silhouette Score:\n",
        "Directly measures how well-defined and separated clusters are\n",
        "For business: Better separation means clearer content categories for users and curators\n",
        "Intra-Cluster Similarity / Inter-Cluster Dissimilarity:\n",
        "Not explicitly calculated but implied by other metrics\n",
        "For business: Ensures content within a group is highly similar while different from other groups\n",
        "Cluster Size Distribution:\n",
        "Helps identify dominant content themes vs. niche areas\n",
        "For business: Informs content acquisition strategy and marketing focus\n",
        "Noise Detection Rate (for DBSCAN):\n",
        "Identifies outlier content that doesn't fit existing categories\n",
        "For business: Highlights innovative or hybrid content types that might need special attention\n",
        "Computational Efficiency:\n",
        "Time to train and predict\n",
        "For business: Practical consideration for maintaining up-to-date content organization"
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose K-Means Clustering as the final model.\n",
        "\n",
        "Reasons:\n",
        "\n",
        "Highest Silhouette Score: Outperformed both Hierarchical Clustering and DBSCAN in cluster separation quality\n",
        "Scalability: More efficient for large datasets compared to Hierarchical Clustering\n",
        "Clear Cluster Assignment: Unlike DBSCAN, every item belongs to exactly one cluster\n",
        "Interpretable Results: Well-defined centroids make it easier to understand what defines each cluster\n",
        "Business Utility: The fixed number of clusters aligns well with practical content organization needs\n",
        "Parameter Stability: Easier to optimize and reproduce compared to DBSCAN's density-based approach\n",
        "While DBSCAN provided valuable insights about outliers and Hierarchical Clustering revealed potential subcategories, K-Means offered the best balance of performance, interpretability, and business applicability."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "\n",
        "# Convert to dense array for SHAP (warning: memory intensive)\n",
        "dense_features = final_features.toarray()\n",
        "\n",
        "# Use KMeans model with optimal clusters\n",
        "explainer = shap.KernelExplainer(lambda x: kmeans.predict(x), dense_features[:100])\n",
        "shap_values = explainer.shap_values(dense_features[:100])\n",
        "\n",
        "# Plot summary\n",
        "plt.figure(figsize=(10, 6))\n",
        "shap.summary_plot(shap_values, dense_features[:100], feature_names=tfidf_vectorizer.get_feature_names_out())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XtzWtx5q5TkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_terms_per_cluster(tfidf_matrix, clusters, feature_names, n_terms=10):\n",
        "    df_weights = pd.DataFrame()\n",
        "\n",
        "    for cluster in range(clusters.max() + 1):\n",
        "        mask = clusters == cluster\n",
        "        cluster_words = tfidf_matrix[mask].toarray().sum(axis=0)\n",
        "\n",
        "        top_words_idx = cluster_words.argsort()[-n_terms:][::-1]\n",
        "        top_words = [feature_names[i] for i in top_words_idx]\n",
        "\n",
        "        df_weights[f'Cluster {cluster}'] = top_words\n",
        "\n",
        "    return df_weights\n",
        "\n",
        "# Get top terms for each cluster\n",
        "top_terms = get_top_terms_per_cluster(tfidf_matrix, cluster_labels, tfidf_vectorizer.get_feature_names_out())\n",
        "print(\"Top terms per cluster:\")\n",
        "print(top_terms)"
      ],
      "metadata": {
        "id": "eA4f4kTU5TOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File\n",
        "import joblib\n",
        "\n",
        "# Save the best performing model\n",
        "joblib.dump(final_kmeans, 'netflix_content_clustering_model.pkl')\n",
        "\n",
        "# Save the TF-IDF vectorizer and SVD transformer for future use\n",
        "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')\n",
        "joblib.dump(svd, 'svd_transformer.pkl')\n",
        "joblib.dump(scaler, 'scaler.pkl')\n",
        "\n",
        "print(\"Models saved successfully.\")"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data.\n",
        "# Load and predict on new data for sanity check\n",
        "def predict_content_cluster(title, description, release_year):\n",
        "    # Create a dummy entry with the provided info\n",
        "    new_data = pd.DataFrame({\n",
        "        'title': [title],\n",
        "        'director': ['unknown_director'],\n",
        "        'cast': ['unknown_cast'],\n",
        "        'country': ['unknown_country'],\n",
        "        'listed_in': ['unknown_listed_in'],\n",
        "        'description': [description]\n",
        "    })\n",
        "\n",
        "    # Preprocess text\n",
        "    for col in ['title', 'director', 'cast', 'country', 'listed_in', 'description']:\n",
        "        new_data[f'{col}_clean'] = new_data[col].astype(str).apply(preprocess_text)\n",
        "\n",
        "    # Combine features\n",
        "    text_fields = [f\"{col}_clean\" for col in ['title', 'director', 'cast', 'country', 'listed_in', 'description']]\n",
        "    new_data['combined_features'] = new_data[text_fields].agg(' '.join, axis=1)\n",
        "\n",
        "    # Transform features\n",
        "    tfidf = tfidf_vectorizer.transform(new_data['combined_features'])\n",
        "    reduced_tfidf = svd.transform(tfidf)\n",
        "\n",
        "    # Scale numerical features\n",
        "    scaled_numeric = scaler.transform([[release_year]])\n",
        "\n",
        "    # Combine features\n",
        "    final_features = sp.hstack([reduced_tfidf, scaled_numeric])\n",
        "\n",
        "    # Predict cluster\n",
        "    return final_kmeans.predict(final_features)[0]\n",
        "\n",
        "# Test prediction\n",
        "test_title = \"Sample Movie\"\n",
        "test_description = \"Exciting action film with international spies and high-tech gadgets\"\n",
        "test_year = 2022"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hurrah! I have successfully completed this Machine Learning Capstone Project analyzing Netflix's movie and TV show content.\n",
        "\n",
        "The project employed three clustering algorithms - K-Means, Hierarchical Clustering, and DBSCAN - to uncover patterns and natural groupings in Netflix's content library. Through rigorous text preprocessing, feature engineering, and model comparison, I found that K-Means clustering provided the most effective organization of content.\n",
        "\n",
        "Key achievements:\n",
        "\n",
        "Comprehensive text preprocessing including contraction expansion, lemmatization, and contextual cleaning\n",
        "Effective feature engineering combining textual and numerical features\n",
        "Successful application of TF-IDF and dimensionality reduction for high-dimensional text data\n",
        "Systematic model comparison revealing K-Means as the optimal clustering solution\n",
        "Interpretation of clusters through term frequency analysis\n",
        "Creation of a deployable model for predicting content categories\n",
        "The resulting model provides actionable insights for Netflix:\n",
        "\n",
        "Improved content organization and navigation\n",
        "Enhanced recommendation systems through better content categorization\n",
        "Data-driven decisions for content acquisition and production\n",
        "Identification of content gaps and market opportunities\n",
        "This project demonstrates the power of unsupervised learning in organizing complex, real-world datasets and extracting meaningful patterns that can drive business decisions in the entertainment industry."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}